{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Normalization\n",
    "\n",
    "def norm_manuak(data):\n",
    "     n_max = max(data)\n",
    "     n_min = min(data)\n",
    "     len_arr = len(data) #length of the entire data\n",
    "\n",
    "     for i in range(0, len_arr):\n",
    "          data[i] = (data[i] - n_min) / (n_max - n_min)\n",
    "\n",
    "     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.75, 1.0, 0.5]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nilai = [10, 13, 14, 12]\n",
    "nilai_norm = norm_manuak(nilai)\n",
    "nilai_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [10, 15, 13, 14, 20, 30, 40, 28]\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[10, 11], [11, 22]]\n",
    "data = np.asarray(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "     [100, 0.01],\n",
    "     []\n",
    "]\n",
    "data = np.asarray(data)\n",
    "data.shape\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled = scaler.fit_transform(data)\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_man(data):\n",
    "     n_mean = np.mean(data)\n",
    "     n_std = np.std(data)\n",
    "     l_data = len(data)\n",
    "\n",
    "     for i in range(0, l_data):\n",
    "          data[i] = (data[i] - n_mean) / n_std\n",
    "\n",
    "     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.3344363375534447,\n",
       " -1.106605743337003,\n",
       " 1.627361387260298,\n",
       " -0.42311396068767765,\n",
       " -0.1952833664712359,\n",
       " 0.48820841617808935,\n",
       " 0.9438696046109728]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nilai = [10, 11,23,14,15,18,20]\n",
    "nilai_std = std_man(nilai)\n",
    "nilai_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data asli: \n",
      "[[100, 0.001], [8, 0.05], [50, 0.005], [88, 0.07], [4, 0.1]]\n",
      "Data transform hasil standarisasi: \n",
      "[[ 1.26398112 -1.16389967]\n",
      " [-1.06174414  0.12639634]\n",
      " [ 0.         -1.05856939]\n",
      " [ 0.96062565  0.65304778]\n",
      " [-1.16286263  1.44302493]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import asarray\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = ([[100, 0.001],\n",
    "          [8, 0.05],\n",
    "          [50, 0.005],\n",
    "          [88, 0.07],\n",
    "          [4, 0.1]])\n",
    "\n",
    "print (\"Data asli: \")\n",
    "print(data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print (\"Data transform hasil standarisasi: \")\n",
    "\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [4.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "oe = OrdinalEncoder()\n",
    "\n",
    "poli = [\n",
    "     [\"POLINEMA\"],\n",
    "     [\"PENS\"],\n",
    "     [\"PNJ\"],\n",
    "     [\"POLBAN\"],\n",
    "     [\"POLMANBABEL\"]\n",
    "]\n",
    "\n",
    "poli_enc = oe.fit_transform(poli)\n",
    "poli_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "oh = OneHotEncoder()\n",
    "\n",
    "poli = [\n",
    "     [\"POLINEMA\"],\n",
    "     [\"PENS\"],\n",
    "     [\"PNJ\"],\n",
    "     [\"POLBAN\"],\n",
    "     [\"POLMANBABEL\"]\n",
    "]\n",
    "\n",
    "poli_enc = oh.fit_transform(poli)\n",
    "poli_enc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "dv = DictVectorizer()\n",
    "\n",
    "dt = [\n",
    "          {'name' : 'POLINEMA'},\n",
    "          {'name' : 'PENS'},\n",
    "          {'name' : 'PNJ'},\n",
    "          {'name' : 'POLBAN'},\n",
    "          {'name' : 'POLIJE'}\n",
    "\n",
    "]\n",
    "\n",
    "poli_dv = dv.fit_transform(dt)\n",
    "poli_dv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'the house had a tiny little mouse',\n",
    "     'the cat saw the mouse',\n",
    "     'the mouse ran away from the house',\n",
    "     'the cat finally ate the mouse',\n",
    "     'the end of the mouse story'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 12)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vector = TfidfVectorizer(stop_words='english')\n",
    "result = vector.fit_transform(corpus)\n",
    "result = result.toarray()\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ate',\n",
       " 'away',\n",
       " 'cat',\n",
       " 'end',\n",
       " 'finally',\n",
       " 'house',\n",
       " 'little',\n",
       " 'mouse',\n",
       " 'ran',\n",
       " 'saw',\n",
       " 'story',\n",
       " 'tiny']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.        , 0.        , 0.        ],\n",
       "        [0.78722298, 0.        , 0.61666846],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.53802897, 0.84292635, 0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "     'This is the first document',\n",
    "     'This document is the second document.',\n",
    "     'This is the third one',\n",
    "     'And this is the fourth document ?'\n",
    "]\n",
    "\n",
    "vector = TfidfVectorizer(stop_words='english')\n",
    "response = vector.fit_transform(corpus)\n",
    "response.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2bc50807b50fc9c50b27dc9f778eb3a8b7bf061a06d2aafe1102622ff0f45480"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
